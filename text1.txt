We first compare our approach against recent methodsfor unpaired image-to-image translation on paired datasetswhere ground truth input-output pairs are available for evaluation.We then study the importance of both the adversarialloss and the cycle consistency loss and compare our fullmethod against several variants. Finally, we demonstratethe generality of our algorithm on a wide range of applicationswhere paired data does not exist. For brevity, we referto our method as CycleGAN. The PyTorch and Torch code,models, and full results can be found at our website.